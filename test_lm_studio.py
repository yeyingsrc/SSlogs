#!/usr/bin/env python3
"""
LM Studioè¿æ¥æµ‹è¯•è„šæœ¬
ç”¨äºè¯Šæ–­æ¨¡å‹åç§°å’Œè¿æ¥é—®é¢˜
"""

import requests
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.lm_studio_connector import LMStudioConnector, LMStudioConfig
from core.ai_config_manager import get_ai_config_manager

def test_lm_studio_connection():
    """æµ‹è¯•LM Studioè¿æ¥"""
    print("ğŸ” LM Studioè¿æ¥è¯Šæ–­")
    print("=" * 50)

    # 1. æµ‹è¯•åŸºæœ¬è¿æ¥
    print("\n1. æµ‹è¯•åŸºæœ¬è¿æ¥...")
    try:
        response = requests.get("http://127.0.0.1:1234/v1/models", timeout=5)
        if response.status_code == 200:
            models = response.json().get("data", [])
            print(f"âœ… è¿æ¥æˆåŠŸï¼å‘ç° {len(models)} ä¸ªæ¨¡å‹")

            if models:
                print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                for i, model in enumerate(models, 1):
                    model_id = model.get("id", "Unknown")
                    print(f"  {i}. {model_id}")
            else:
                print("âŒ æœªå‘ç°å¯ç”¨æ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨LM Studioä¸­åŠ è½½äº†æ¨¡å‹")
                return False
        else:
            print(f"âŒ è¿æ¥å¤±è´¥: HTTP {response.status_code}")
            print(f"å“åº”: {response.text}")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¼‚å¸¸: {e}")
        print("è¯·ç¡®ä¿:")
        print("  â€¢ LM Studioæ­£åœ¨è¿è¡Œ")
        print("  â€¢ æœ¬åœ°æœåŠ¡å™¨å·²å¯åŠ¨ (ç«¯å£1234)")
        print("  â€¢ å·²åŠ è½½è‡³å°‘ä¸€ä¸ªæ¨¡å‹")
        return False

    # 2. æµ‹è¯•é…ç½®ç®¡ç†å™¨
    print("\n2. æµ‹è¯•é…ç½®ç®¡ç†å™¨...")
    try:
        config_manager = get_ai_config_manager()
        lm_config = config_manager.get_lm_studio_config()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  ä¸»æœº: {lm_config.host}:{lm_config.port}")
        print(f"  APIåœ°å€: {lm_config.api.base_url}")
        print(f"  é¦–é€‰æ¨¡å‹: {lm_config.model.preferred_model or 'æœªè®¾ç½®'}")
        print(f"  æ¨¡å‹æ˜ å°„: {len(lm_config.model.model_mapping)}ä¸ª")

        if lm_config.model.model_mapping:
            print("  ğŸ“ æ˜ å°„å…³ç³»:")
            for actual, mapped in lm_config.model.model_mapping.items():
                print(f"    {actual} â†’ {mapped}")
        else:
            print("  âš ï¸ æ— æ¨¡å‹åç§°æ˜ å°„")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

    # 3. æµ‹è¯•è¿æ¥å™¨
    print("\n3. æµ‹è¯•LM Studioè¿æ¥å™¨...")
    try:
        connector = LMStudioConnector(lm_config)

        if connector.check_connection():
            print("âœ… è¿æ¥å™¨æµ‹è¯•æˆåŠŸ")
            print(f"  å¯ç”¨æ¨¡å‹: {len(connector.available_models)}ä¸ª")
            print(f"  å½“å‰æ¨¡å‹: {connector.current_model or 'æœªè®¾ç½®'}")
        else:
            print("âŒ è¿æ¥å™¨æµ‹è¯•å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å™¨å¼‚å¸¸: {e}")
        return False

    # 4. æµ‹è¯•æ¨¡å‹åç§°
    print("\n4. æµ‹è¯•æ¨¡å‹åç§°...")
    test_model_name = "openai/gpt-oss-20b"
    print(f"æµ‹è¯•æ¨¡å‹åç§°: {test_model_name}")

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ˜ å°„åç§°
    actual_model_id = lm_config.get_actual_model_id(test_model_name)
    print(f"æ˜ å°„åçš„å®é™…ID: {actual_model_id}")

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if actual_model_id in connector.available_models:
        print(f"âœ… æ¨¡å‹ {actual_model_id} å¯ç”¨")
    else:
        print(f"âŒ æ¨¡å‹ {actual_model_id} ä¸å¯ç”¨")
        print("å¯ç”¨æ¨¡å‹:")
        for model in connector.available_models:
            print(f"  â€¢ {model}")

        # å»ºè®®è§£å†³æ–¹æ¡ˆ
        if connector.available_models:
            suggested_model = connector.available_models[0]
            print(f"\nğŸ’¡ å»ºè®®:")
            print(f"1. ä½¿ç”¨å¯ç”¨æ¨¡å‹: {suggested_model}")
            print(f"2. æˆ–åœ¨é…ç½®ä¸­æ·»åŠ æ˜ å°„:")
            print(f"   model_mapping:")
            print(f"     '{suggested_model}': '{test_model_name}'")

    # 5. æµ‹è¯•èŠå¤©è¯·æ±‚
    print("\n5. æµ‹è¯•èŠå¤©è¯·æ±‚...")
    try:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•
        if connector.available_models:
            test_model = connector.available_models[0]
            print(f"ä½¿ç”¨æ¨¡å‹: {test_model}")

            from core.lm_studio_connector import ChatMessage
            messages = [
                ChatMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ç¡®è®¤è¿æ¥æ­£å¸¸")
            ]

            result = connector.chat_completion(
                messages=messages,
                model=test_model,
                max_tokens=50,
                temperature=0.3
            )

            if result:
                print(f"âœ… èŠå¤©æµ‹è¯•æˆåŠŸ")
                print(f"å›å¤: {result[:100]}...")
            else:
                print("âŒ èŠå¤©æµ‹è¯•å¤±è´¥")
        else:
            print("âŒ æ— å¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•")
    except Exception as e:
        print(f"âŒ èŠå¤©æµ‹è¯•å¼‚å¸¸: {e}")

    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LM Studioè¿æ¥è¯Šæ–­å·¥å…·")
    print("=" * 60)

    try:
        success = test_lm_studio_connection()

        print("\n" + "=" * 60)
        if success:
            print("âœ… è¯Šæ–­å®Œæˆ")
        else:
            print("âŒ è¯Šæ–­å‘ç°é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯è¿›è¡Œä¿®å¤")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ è¯Šæ–­è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()