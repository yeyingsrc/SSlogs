#!/usr/bin/env python3
"""
SSlogs æµ‹è¯•è¿è¡Œè„šæœ¬
æ‰§è¡Œå®Œæ•´çš„è§„åˆ™æµ‹è¯•æµç¨‹
"""

import os
import sys
import time
import argparse
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from tests.test_framework import TestFramework

def main():
    parser = argparse.ArgumentParser(description='SSlogs è§„åˆ™æµ‹è¯•å·¥å…·')
    parser.add_argument('--rule-dir', default='rules', help='è§„åˆ™ç›®å½•è·¯å¾„')
    parser.add_argument('--generate-data', action='store_true', help='ç”Ÿæˆæµ‹è¯•æ•°æ®')
    parser.add_argument('--run-tests', action='store_true', help='è¿è¡Œæµ‹è¯•')
    parser.add_argument('--generate-report', action='store_true', help='ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š')
    parser.add_argument('--category', help='æŒ‡å®šæµ‹è¯•ç±»åˆ« (ai_ml, threat_intel, zero_trust, etc.)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•æ¡†æ¶
    framework = TestFramework(args.rule_dir)

    try:
        # æ­¥éª¤1ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®
        if args.generate_data or not any([args.run_tests, args.generate_report]):
            print("ğŸš€ å¼€å§‹ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            test_logs = framework.generate_test_logs()
            print(f"âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå…± {len(test_logs)} ä¸ªç±»åˆ«")

        # æ­¥éª¤2ï¼šè¿è¡Œæµ‹è¯•
        if args.run_tests or not any([args.generate_data, args.generate_report]):
            print("ğŸ” å¼€å§‹åŠ è½½è§„åˆ™å¼•æ“...")
            rule_engine = framework.load_rule_engine()

            print("ğŸ§ª å¼€å§‹è¿è¡Œè§„åˆ™æµ‹è¯•...")
            if args.generate_data:
                results = framework.run_tests(rule_engine, test_logs)
            else:
                # åŠ è½½ç°æœ‰çš„æµ‹è¯•æ•°æ®
                test_logs = {}
                test_data_dir = Path(__file__).parent / "test_data"
                for test_file in test_data_dir.glob("*_test_logs.json"):
                    category = test_file.stem.replace("_test_logs", "")
                    with open(test_file, 'r', encoding='utf-8') as f:
                        test_logs[category] = json.load(f)

                results = framework.run_tests(rule_engine, test_logs)

            print(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ€»è®¡ {framework.test_stats['total_tests']} ä¸ªæµ‹è¯•")
            print(f"   é€šè¿‡: {framework.test_stats['passed_tests']}")
            print(f"   å¤±è´¥: {framework.test_stats['failed_tests']}")
            print(f"   é”™è¯¯: {framework.test_stats['error_tests']}")

        # æ­¥éª¤3ï¼šç”ŸæˆæŠ¥å‘Š
        if args.generate_report or not any([args.generate_data, args.run_tests]):
            print("ğŸ“Š å¼€å§‹ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
            if args.run_tests:
                report_file = framework.generate_report(results)
            else:
                # åŠ è½½ç°æœ‰çš„æµ‹è¯•ç»“æœï¼ˆå¯»æ‰¾test_resultsæ–‡ä»¶ï¼Œä¸æ˜¯validation_reportï¼‰
                results_dir = Path(__file__).parent / "results"
                test_result_files = list(results_dir.glob("test_results_*.json"))
                if not test_result_files:
                    print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•")
                    return 1

                latest_result = max(test_result_files, key=lambda x: x.stat().st_mtime)
                print(f"ğŸ“‚ åŠ è½½æµ‹è¯•ç»“æœæ–‡ä»¶: {latest_result.name}")
                with open(latest_result, 'r', encoding='utf-8') as f:
                    results = json.load(f)

                # é‡æ–°æ„å»ºç»Ÿè®¡ä¿¡æ¯
                framework.test_stats = {
                    'total_tests': sum(len(category_results) for category_results in results.values()),
                    'passed_tests': sum(1 for category_results in results.values()
                                      for result in category_results if result.get('success', False)),
                    'failed_tests': sum(1 for category_results in results.values()
                                      for result in category_results if not result.get('success', False) and 'error' not in result),
                    'error_tests': sum(1 for category_results in results.values()
                                     for result in category_results if 'error' in result),
                    'test_results': results
                }

                report_file = framework.generate_report(results)

            print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        if args.verbose and args.run_tests:
            print("\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
            for category, category_results in results.items():
                print(f"\nğŸ”¸ {category}:")
                for result in category_results:
                    if result.get('success', False):
                        print(f"  âœ… {result['test_id']}: åŒ¹é…åˆ° {result['matches']} ä¸ªè§„åˆ™")
                        for rule_name in result['matched_rules']:
                            print(f"     - {rule_name}")
                    else:
                        print(f"  âŒ {result['test_id']}: æœªåŒ¹é…åˆ°è§„åˆ™")
                        if 'error' in result:
                            print(f"     é”™è¯¯: {result['error']}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())