#!/usr/bin/env python3
"""
åº”æ€¥æ—¥å¿—æ”¶é›†å·¥å…· - ä¼˜åŒ–ç‰ˆæœ¬ v2.0
ç”¨äºå¿«é€ŸæŠ“å–access.logå¹¶æ‰“åŒ…åˆ°tmpæ–‡ä»¶å¤¹
æ”¯æŒå¹¶å‘å¤„ç†ã€é…ç½®ç®¡ç†ã€è¯¦ç»†æ—¥å¿—è®°å½•

ç‰¹æ€§:
- ğŸš€ å¹¶å‘æ–‡ä»¶å¤åˆ¶ï¼Œæå‡å¤„ç†é€Ÿåº¦
- âš™ï¸  JSONé…ç½®æ–‡ä»¶æ”¯æŒ
- ğŸ“Š è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡å’Œå‹ç¼©åˆ†æ  
- ğŸ—‚ï¸  æ™ºèƒ½è·¯å¾„æœç´¢ï¼Œæ”¯æŒæ·±åº¦é™åˆ¶
- ğŸ“ å®Œæ•´çš„æ—¥å¿—è®°å½•ç³»ç»Ÿ
- ğŸ”’ è‡ªåŠ¨sudoæƒé™å¤„ç†
- ğŸ’¾ æµå¼å¤åˆ¶å¤§æ–‡ä»¶ï¼ŒèŠ‚çœå†…å­˜
- ğŸ“¦ é«˜æ•ˆå‹ç¼©ï¼ŒåŒ…å«å…ƒæ•°æ®ä¿¡æ¯

ä½œè€…: Emergency Response Team
ç‰ˆæœ¬: 2.0.0
æ›´æ–°: 2025-07-28
"""

import os
import sys
import shutil
import gzip
import tarfile
import subprocess
import logging
import json
import glob
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import time

VERSION = "2.0.0"
AUTHOR = "Emergency Response Team"
UPDATE_DATE = "2025-07-28"

def print_banner():
    """æ‰“å°å·¥å…·æ¨ªå¹…"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸš¨ åº”æ€¥æ—¥å¿—æ”¶é›†å·¥å…· v{VERSION}                    â•‘
â•‘                     Emergency Log Collector                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸš€ å¿«é€Ÿæ”¶é›†ç³»ç»Ÿä¸­çš„access.logæ–‡ä»¶                              â•‘
â•‘ ğŸ“¦ è‡ªåŠ¨æ‰“åŒ…å‹ç¼©ï¼Œä¾¿äºä¼ è¾“å’Œåˆ†æ                                  â•‘
â•‘ âš™ï¸  æ”¯æŒå¹¶å‘å¤„ç†ï¼Œæ™ºèƒ½æƒé™ç®¡ç†                                   â•‘
â•‘ ğŸ“Š æä¾›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å’Œå…ƒæ•°æ®                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def print_help_info():
    """æ‰“å°è¯¦ç»†å¸®åŠ©ä¿¡æ¯"""
    help_text = f"""
ğŸ”§ ä½¿ç”¨æ–¹æ³•:
    python emergency_log_collector.py [é€‰é¡¹]

ğŸ“‹ å‘½ä»¤è¡Œé€‰é¡¹:
    -h, --help              æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    -c, --config FILE       æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)
    -w, --max-workers N     è®¾ç½®æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 4)
    -s, --max-size N        è®¾ç½®å•æ–‡ä»¶æœ€å¤§å¤§å°MB (é»˜è®¤: 100)
    -d, --depth N           è®¾ç½®æœç´¢ç›®å½•æ·±åº¦ (é»˜è®¤: 3)
    -v, --verbose           å¯ç”¨è¯¦ç»†è¾“å‡º (DEBUGçº§åˆ«æ—¥å¿—)
    --version               æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯

ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:
    # åŸºæœ¬ä½¿ç”¨
    python emergency_log_collector.py
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶
    python emergency_log_collector.py --config /path/to/config.json
    
    # è‡ªå®šä¹‰å‚æ•°
    python emergency_log_collector.py --max-workers 8 --depth 2 --verbose
    
    # é™åˆ¶æ–‡ä»¶å¤§å°
    python emergency_log_collector.py --max-size 50 --verbose

ğŸ“ é…ç½®æ–‡ä»¶æ ¼å¼ (JSON):
{{
    "tmp_dir": "/tmp/emergency_logs",      # ä¸´æ—¶ç›®å½•
    "max_workers": 4,                      # å¹¶å‘çº¿ç¨‹æ•°
    "max_file_size_mb": 100,              # æœ€å¤§æ–‡ä»¶å¤§å°(MB)
    "search_depth": 3,                     # æœç´¢æ·±åº¦
    "log_level": "INFO",                   # æ—¥å¿—çº§åˆ«
    "additional_paths": [                  # é¢å¤–æœç´¢è·¯å¾„
        "/var/log/custom/*.log",
        "/opt/app/logs/access*.log"
    ]
}}

ğŸ” å·¥å…·ä¼šæœç´¢ä»¥ä¸‹ä½ç½®:
    âœ“ /var/log/apache2/access.log*
    âœ“ /var/log/nginx/access.log*
    âœ“ /var/log/httpd/access.log*
    âœ“ /opt/lampp/logs/access_log
    âœ“ å½“å‰ç›®å½•åŠå­ç›®å½•ä¸­çš„ *access*.log* æ–‡ä»¶
    âœ“ ç”¨æˆ·ä¸»ç›®å½•ä¸­çš„ *access*.log* æ–‡ä»¶
    âœ“ é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„é¢å¤–è·¯å¾„

âš¡ æ€§èƒ½ç‰¹æ€§:
    ğŸ”„ å¤šçº¿ç¨‹å¹¶å‘å¤åˆ¶æ–‡ä»¶
    ğŸ’¾ å¤§æ–‡ä»¶æµå¼å¤„ç†ï¼ŒèŠ‚çœå†…å­˜
    ğŸ“ æ™ºèƒ½æ–‡ä»¶å¤§å°è¿‡æ»¤
    ğŸ¯ é™åˆ¶æœç´¢æ·±åº¦ï¼Œé¿å…è¿‡åº¦æ‰«æ
    ğŸ—œï¸  é«˜æ•ˆgzipå‹ç¼©

ğŸ“Š è¾“å‡ºä¿¡æ¯:
    ğŸ“¦ ç”Ÿæˆå‹ç¼©åŒ…: /tmp/access_logs_YYYYMMDD_HHMMSS.tar.gz
    ğŸ“ è¯¦ç»†æ—¥å¿—: /tmp/emergency_collector_YYYYMMDD_HHMMSS.log
    ğŸ“‹ å…ƒæ•°æ®æ–‡ä»¶: logs/collection_metadata.json (åœ¨å‹ç¼©åŒ…å†…)

ğŸ” æƒé™å¤„ç†:
    âœ“ è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶è¯»å–æƒé™
    âœ“ å¯¹æ— æƒé™æ–‡ä»¶å°è¯•sudoå¤åˆ¶
    âœ“ å¤åˆ¶åè‡ªåŠ¨ä¿®æ­£æ–‡ä»¶æ‰€æœ‰è€…

âš ï¸  æ³¨æ„äº‹é¡¹:
    â€¢ éœ€è¦è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨ä¸´æ—¶æ–‡ä»¶å’Œå‹ç¼©åŒ…
    â€¢ æŸäº›ç³»ç»Ÿæ–‡ä»¶å¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™
    â€¢ å¤§æ–‡ä»¶å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    â€¢ å‹ç¼©åŒ…ä¼šåŒ…å«å®Œæ•´çš„ç›®å½•ç»“æ„

ğŸ“ æŠ€æœ¯æ”¯æŒ:
    ä½œè€…: {AUTHOR}
    ç‰ˆæœ¬: v{VERSION}
    æ›´æ–°: {UPDATE_DATE}
    
    å¦‚æœ‰é—®é¢˜è¯·è”ç³»ç³»ç»Ÿç®¡ç†å‘˜æˆ–å®‰å…¨å›¢é˜Ÿ
"""
    print(help_text)

class EmergencyLogCollector:
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.tmp_dir = Path(self.config.get('tmp_dir', '/tmp/emergency_logs'))
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.package_name = f"access_logs_{self.timestamp}.tar.gz"
        self.max_workers = self.config.get('max_workers', 4)
        self.max_file_size = self.config.get('max_file_size_mb', 100) * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.search_depth = self.config.get('search_depth', 3)
        self._setup_logging()
        self.stats = {
            'found_files': 0,
            'copied_files': 0,
            'failed_files': 0,
            'total_size': 0,
            'start_time': time.time()
        }
        
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'tmp_dir': '/tmp/emergency_logs',
            'max_workers': 4,
            'max_file_size_mb': 100,
            'search_depth': 3,
            'log_level': 'INFO',
            'additional_paths': []
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
        
        return default_config
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_level = getattr(logging, self.config.get('log_level', 'INFO').upper())
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(f'/tmp/emergency_collector_{self.timestamp}.log')
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    @lru_cache(maxsize=128)
    def _get_common_log_paths(self) -> List[str]:
        """è·å–å¸¸è§æ—¥å¿—è·¯å¾„ - ç¼“å­˜ç»“æœ"""
        base_paths = [
            "/var/log/apache2/access.log",
            "/var/log/httpd/access.log",
            "/var/log/nginx/access.log",
            "/var/log/apache2/access.log.1",
            "/var/log/httpd/access_log",
            "/var/log/nginx/access.log.1",
            "/var/log/lighttpd/access.log",
            "/var/log/caddy/access.log",
            "/opt/lampp/logs/access_log",
            "/usr/local/apache2/logs/access.log",
            "/var/log/httpd/access.log-*.gz",
            "/var/log/nginx/access.log-*.gz",
            "/var/log/apache2/access.log-*.gz"
        ]
        
        # æ·»åŠ ç”¨æˆ·é…ç½®çš„é¢å¤–è·¯å¾„
        base_paths.extend(self.config.get('additional_paths', []))
        return base_paths
    
    def find_access_logs(self) -> List[Path]:
        """æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„access.logæ–‡ä»¶ - ä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("å¼€å§‹æŸ¥æ‰¾access.logæ–‡ä»¶...")
        found_logs: Set[Path] = set()
        
        # 1. æŸ¥æ‰¾å¸¸è§è·¯å¾„
        common_paths = self._get_common_log_paths()
        for path_str in common_paths:
            try:
                if '*' in path_str:
                    # å¤„ç†é€šé…ç¬¦
                    matches = glob.glob(path_str)
                    found_logs.update(Path(m) for m in matches if Path(m).is_file())
                else:
                    path = Path(path_str)
                    if path.exists() and path.is_file():
                        found_logs.add(path)
            except Exception as e:
                self.logger.warning(f"æ£€æŸ¥è·¯å¾„å¤±è´¥ {path_str}: {e}")
        
        # 2. é™åˆ¶æ·±åº¦çš„é€’å½’æœç´¢
        search_dirs = [Path.cwd(), Path.home()]
        for search_dir in search_dirs:
            try:
                found_logs.update(self._search_logs_with_depth(search_dir, self.search_depth))
            except Exception as e:
                self.logger.warning(f"æœç´¢ç›®å½•å¤±è´¥ {search_dir}: {e}")
        
        # 3. è¿‡æ»¤æ–‡ä»¶å¤§å°
        filtered_logs = []
        for log_file in found_logs:
            try:
                if log_file.stat().st_size <= self.max_file_size:
                    filtered_logs.append(log_file)
                else:
                    self.logger.warning(f"æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡: {log_file} ({log_file.stat().st_size/1024/1024:.1f}MB)")
            except Exception as e:
                self.logger.warning(f"æ£€æŸ¥æ–‡ä»¶å¤§å°å¤±è´¥ {log_file}: {e}")
        
        self.stats['found_files'] = len(filtered_logs)
        self.logger.info(f"æ‰¾åˆ° {len(filtered_logs)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        return filtered_logs
    
    def _search_logs_with_depth(self, root_dir: Path, max_depth: int) -> Set[Path]:
        """é™åˆ¶æ·±åº¦çš„æ—¥å¿—æ–‡ä»¶æœç´¢"""
        found_logs: Set[Path] = set()
        
        def _recursive_search(current_dir: Path, current_depth: int):
            if current_depth > max_depth:
                return
            
            try:
                for item in current_dir.iterdir():
                    if item.is_file() and ('access' in item.name.lower() and 
                                          ('.log' in item.suffix or '.gz' in item.suffixes)):
                        found_logs.add(item)
                    elif item.is_dir() and not item.is_symlink():
                        _recursive_search(item, current_depth + 1)
            except (PermissionError, OSError):
                pass  # å¿½ç•¥æƒé™é”™è¯¯
        
        _recursive_search(root_dir, 0)
        return found_logs
    
    def check_permissions(self, log_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶è¯»å–æƒé™"""
        try:
            return os.access(log_path, os.R_OK) and log_path.is_file()
        except Exception as e:
            self.logger.debug(f"æƒé™æ£€æŸ¥å¤±è´¥ {log_path}: {e}")
            return False
    
    def _copy_single_file(self, log_file: Path) -> Optional[Path]:
        """å¤åˆ¶å•ä¸ªæ–‡ä»¶ - æ”¯æŒå¹¶å‘è°ƒç”¨"""
        if not self.check_permissions(log_file):
            self.logger.warning(f"æ— æƒé™è¯»å–æ–‡ä»¶: {log_file}")
            return None
        
        try:
            # åˆ›å»ºç›¸å¯¹è·¯å¾„ç»“æ„
            rel_path = log_file.relative_to("/") if log_file.is_absolute() else log_file
            dest_path = self.tmp_dir / rel_path
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æµå¼å¤åˆ¶å¤§æ–‡ä»¶
            if log_file.stat().st_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Šä½¿ç”¨æµå¼å¤åˆ¶
                self._stream_copy(log_file, dest_path)
            else:
                shutil.copy2(log_file, dest_path)
            
            file_size = dest_path.stat().st_size
            self.stats['total_size'] += file_size
            self.stats['copied_files'] += 1
            
            self.logger.info(f"å¤åˆ¶æˆåŠŸ: {log_file} -> {dest_path} ({file_size/1024:.1f}KB)")
            return dest_path
            
        except Exception as e:
            self.stats['failed_files'] += 1
            self.logger.error(f"å¤åˆ¶å¤±è´¥: {log_file} - {e}")
            return None
    
    def _stream_copy(self, src: Path, dest: Path, chunk_size: int = 1024*1024):
        """æµå¼å¤åˆ¶å¤§æ–‡ä»¶"""
        with open(src, 'rb') as fsrc, open(dest, 'wb') as fdest:
            while True:
                chunk = fsrc.read(chunk_size)
                if not chunk:
                    break
                fdest.write(chunk)
    
    def copy_logs(self, log_files: List[Path]) -> List[Path]:
        """å¹¶å‘å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
        self.tmp_dir.mkdir(parents=True, exist_ok=True)
        copied_files = []
        
        self.logger.info(f"å¼€å§‹å¤åˆ¶ {len(log_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤åˆ¶
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰å¤åˆ¶ä»»åŠ¡
            future_to_file = {executor.submit(self._copy_single_file, log_file): log_file 
                            for log_file in log_files}
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                result = future.result()
                if result:
                    copied_files.append(result)
        
        self.logger.info(f"å¤åˆ¶å®Œæˆ: æˆåŠŸ {len(copied_files)} ä¸ªï¼Œå¤±è´¥ {self.stats['failed_files']} ä¸ª")
        return copied_files
    
    def create_package(self, log_files: List[Path]) -> Optional[Path]:
        """åˆ›å»ºå‹ç¼©åŒ… - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not log_files:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°å¯å¤åˆ¶çš„æ—¥å¿—æ–‡ä»¶")
            return None
        
        package_path = Path("/tmp") / self.package_name
        
        try:
            self.logger.info(f"å¼€å§‹åˆ›å»ºå‹ç¼©åŒ…: {package_path}")
            
            with tarfile.open(package_path, "w:gz", compresslevel=6) as tar:
                # æ·»åŠ å…ƒæ•°æ®æ–‡ä»¶
                self._create_metadata_file()
                tar.add(self.tmp_dir, arcname="logs")
            
            package_size = package_path.stat().st_size
            compression_ratio = (self.stats['total_size'] / package_size) if package_size > 0 else 0
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(self.tmp_dir, ignore_errors=True)
            
            self.logger.info(f"å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {package_path}")
            self.logger.info(f"å‹ç¼©å‰: {self.stats['total_size']/1024/1024:.2f}MB, "
                           f"å‹ç¼©å: {package_size/1024/1024:.2f}MB, "
                           f"å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
            return package_path
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {e}")
            return None
    
    def _create_metadata_file(self):
        """åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶"""
        metadata = {
            'collection_time': datetime.now().isoformat(),
            'hostname': os.uname().nodename,
            'total_files': self.stats['copied_files'],
            'total_size_bytes': self.stats['total_size'],
            'collection_duration_seconds': time.time() - self.stats['start_time'],
            'config': self.config
        }
        
        metadata_path = self.tmp_dir / 'collection_metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def print_summary(self):
        """æ‰“å°æ”¶é›†æ‘˜è¦"""
        duration = time.time() - self.stats['start_time']
        print("\n" + "=" * 60)
        print("ğŸ“Š æ”¶é›†æ‘˜è¦")
        print("=" * 60)
        print(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ“ å‘ç°æ–‡ä»¶: {self.stats['found_files']} ä¸ª")
        print(f"âœ… æˆåŠŸå¤åˆ¶: {self.stats['copied_files']} ä¸ª")
        print(f"âŒ å¤åˆ¶å¤±è´¥: {self.stats['failed_files']} ä¸ª")
        print(f"ğŸ“¦ æ€»å¤§å°: {self.stats['total_size']/1024/1024:.2f} MB")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {(self.stats['total_size']/1024/1024)/duration:.2f} MB/s")
        print("=" * 60)
    
    def run_sudo_copy(self, log_files: List[Path]) -> List[Path]:
        """ä½¿ç”¨sudoæƒé™å¤åˆ¶éœ€è¦ç‰¹æƒçš„æ—¥å¿— - ä¼˜åŒ–ç‰ˆæœ¬"""
        sudo_logs = [log_file for log_file in log_files if not self.check_permissions(log_file)]
        
        if not sudo_logs:
            return []
        
        self.logger.info(f"æ£€æµ‹åˆ° {len(sudo_logs)} ä¸ªéœ€è¦sudoæƒé™çš„æ–‡ä»¶")
        
        # æ£€æŸ¥sudoå¯ç”¨æ€§
        try:
            subprocess.run(["sudo", "-n", "true"], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            self.logger.warning("sudoéœ€è¦å¯†ç æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡ç‰¹æƒæ–‡ä»¶å¤åˆ¶")
            return []
        
        sudo_copied = []
        for log_file in sudo_logs:
            try:
                dest_path = self.tmp_dir / log_file.name
                cmd = ["sudo", "cp", str(log_file), str(dest_path)]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0:
                    # ä¿®æ”¹æ–‡ä»¶æƒé™
                    subprocess.run(["sudo", "chown", f"{os.getuid()}:{os.getgid()}", str(dest_path)], 
                                 capture_output=True)
                    sudo_copied.append(dest_path)
                    self.stats['copied_files'] += 1
                    self.logger.info(f"sudoå¤åˆ¶æˆåŠŸ: {log_file}")
                else:
                    self.stats['failed_files'] += 1
                    self.logger.error(f"sudoå¤åˆ¶å¤±è´¥: {log_file} - {result.stderr}")
            except subprocess.TimeoutExpired:
                self.logger.error(f"sudoå¤åˆ¶è¶…æ—¶: {log_file}")
            except Exception as e:
                self.logger.error(f"sudoå‘½ä»¤å¤±è´¥: {e}")
        
        return sudo_copied
    
    def run(self) -> bool:
        """ä¸»è¿è¡Œå‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            self.logger.info("åº”æ€¥æ—¥å¿—æ”¶é›†å·¥å…·å¯åŠ¨")
            print("ğŸš¨ åº”æ€¥æ—¥å¿—æ”¶é›†å·¥å…·å¯åŠ¨...")
            print("=" * 50)
            
            # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
            log_files = self.find_access_logs()
            
            if not log_files:
                self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•access.logæ–‡ä»¶")
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•access.logæ–‡ä»¶")
                return False
            
            print(f"ğŸ“ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
            
            # æ˜¾ç¤ºæƒé™çŠ¶æ€
            accessible_files = [f for f in log_files if self.check_permissions(f)]
            restricted_files = [f for f in log_files if not self.check_permissions(f)]
            
            if accessible_files:
                print(f"âœ… å¯ç›´æ¥è®¿é—®: {len(accessible_files)} ä¸ª")
            if restricted_files:
                print(f"ğŸ” éœ€è¦ç‰¹æƒè®¿é—®: {len(restricted_files)} ä¸ª")
            
            # å¤åˆ¶æœ‰æƒé™çš„æ–‡ä»¶
            copied_files = self.copy_logs(accessible_files) if accessible_files else []
            
            # å°è¯•ä½¿ç”¨sudoå¤åˆ¶éœ€è¦ç‰¹æƒçš„æ–‡ä»¶
            if restricted_files:
                sudo_copied = self.run_sudo_copy(restricted_files)
                copied_files.extend(sudo_copied)
            
            if not copied_files:
                self.logger.error("æ²¡æœ‰æˆåŠŸå¤åˆ¶ä»»ä½•æ—¥å¿—æ–‡ä»¶")
                print("âŒ æ²¡æœ‰æˆåŠŸå¤åˆ¶ä»»ä½•æ—¥å¿—æ–‡ä»¶")
                return False
            
            # åˆ›å»ºå‹ç¼©åŒ…
            package_path = self.create_package(copied_files)
            
            if package_path:
                print("\nğŸ‰ åº”æ€¥æ—¥å¿—æ”¶é›†å®Œæˆï¼")
                self.print_summary()
                print(f"ğŸ“¦ å‹ç¼©åŒ…ä½ç½®: {package_path}")
                print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: /tmp/emergency_collector_{self.timestamp}.log")
                print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
                print(f"   è§£å‹å‘½ä»¤: tar -xzf {package_path}")
                print(f"   æŸ¥çœ‹å†…å®¹: tar -tzf {package_path}")
                print(f"   æŸ¥çœ‹å…ƒæ•°æ®: tar -xzf {package_path} logs/collection_metadata.json -O")
                return True
            else:
                self.logger.error("åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥")
                print("âŒ åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
            return False

def generate_config_template():
    """ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿"""
    template = {
        "tmp_dir": "/tmp/emergency_logs",
        "max_workers": 4,
        "max_file_size_mb": 100,
        "search_depth": 3,
        "log_level": "INFO",
        "additional_paths": [
            "/var/log/custom/*.log",
            "/opt/app/logs/access*.log",
            "/home/*/logs/*access*.log"
        ]
    }
    
    config_path = "emergency_collector_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=4, ensure_ascii=False)
    
    print(f"âœ… é…ç½®æ–‡ä»¶æ¨¡æ¿å·²ç”Ÿæˆ: {config_path}")
    print("ğŸ“ è¯·æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®å‚æ•°ï¼Œç„¶åä½¿ç”¨ --config å‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶")
    return config_path

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        prog='emergency_log_collector.py',
        description=f'åº”æ€¥æ—¥å¿—æ”¶é›†å·¥å…· v{VERSION} - å¿«é€Ÿæ”¶é›†å’Œæ‰“åŒ…access.logæ–‡ä»¶',
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s                                    # ä½¿ç”¨é»˜è®¤é…ç½®
  %(prog)s --verbose                          # å¯ç”¨è¯¦ç»†è¾“å‡º
  %(prog)s --config myconfig.json             # ä½¿ç”¨é…ç½®æ–‡ä»¶
  %(prog)s --max-workers 8 --depth 2          # è‡ªå®šä¹‰å‚æ•°
  %(prog)s --generate-config                  # ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿
  %(prog)s --help-detailed                    # æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©

æ›´å¤šä¿¡æ¯è¯·è®¿é—®: https://github.com/emergency-response/log-collector
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--version', action='version', 
                       version=f'%(prog)s v{VERSION} by {AUTHOR} ({UPDATE_DATE})')
    
    parser.add_argument('--config', '-c', type=str, metavar='FILE',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    
    parser.add_argument('--max-workers', '-w', type=int, default=4, metavar='N',
                       help='æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: %(default)s)')
    
    parser.add_argument('--max-size', '-s', type=int, default=100, metavar='N',
                       help='å•æ–‡ä»¶æœ€å¤§å¤§å°MB (é»˜è®¤: %(default)s)')
    
    parser.add_argument('--depth', '-d', type=int, default=3, metavar='N',
                       help='æœç´¢ç›®å½•æ·±åº¦ (é»˜è®¤: %(default)s)')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='å¯ç”¨è¯¦ç»†è¾“å‡º (DEBUGçº§åˆ«æ—¥å¿—)')
    
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºé”™è¯¯ä¿¡æ¯')
    
    parser.add_argument('--no-banner', action='store_true',
                       help='ä¸æ˜¾ç¤ºç¨‹åºæ¨ªå¹…')
    
    parser.add_argument('--generate-config', action='store_true',
                       help='ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿å¹¶é€€å‡º')
    
    parser.add_argument('--help-detailed', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©ä¿¡æ¯')
    
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼Œåªæ˜¾ç¤ºä¼šæ”¶é›†çš„æ–‡ä»¶ï¼Œä¸å®é™…å¤åˆ¶')
    
    args = parser.parse_args()
    
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if args.help_detailed:
        print_help_info()
        return 0
    
    if args.generate_config:
        generate_config_template()
        return 0
    
    # æ˜¾ç¤ºæ¨ªå¹…
    if not args.no_banner and not args.quiet:
        print_banner()
    
    # åŠ¨æ€é…ç½®
    config_override = {}
    if args.max_workers:
        config_override['max_workers'] = args.max_workers
    if args.max_size:
        config_override['max_file_size_mb'] = args.max_size
    if args.depth:
        config_override['search_depth'] = args.depth
    if args.verbose:
        config_override['log_level'] = 'DEBUG'
    elif args.quiet:
        config_override['log_level'] = 'ERROR'
    
    try:
        collector = EmergencyLogCollector(args.config)
        
        # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
        collector.config.update(config_override)
        collector._setup_logging()  # é‡æ–°è®¾ç½®æ—¥å¿—çº§åˆ«
        
        # è¯•è¿è¡Œæ¨¡å¼
        if args.dry_run:
            print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ - æ‰«æå¯æ”¶é›†çš„æ–‡ä»¶:")
            print("=" * 50)
            log_files = collector.find_access_logs()
            
            if not log_files:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•access.logæ–‡ä»¶")
                return 1
            
            accessible_files = [f for f in log_files if collector.check_permissions(f)]
            restricted_files = [f for f in log_files if not collector.check_permissions(f)]
            
            total_size = 0
            print(f"\nâœ… å¯ç›´æ¥è®¿é—®çš„æ–‡ä»¶ ({len(accessible_files)} ä¸ª):")
            for i, file_path in enumerate(accessible_files, 1):
                try:
                    size = file_path.stat().st_size
                    total_size += size
                    print(f"  {i:2d}. {file_path} ({size/1024:.1f}KB)")
                except:
                    print(f"  {i:2d}. {file_path} (å¤§å°æœªçŸ¥)")
            
            if restricted_files:
                print(f"\nğŸ” éœ€è¦sudoæƒé™çš„æ–‡ä»¶ ({len(restricted_files)} ä¸ª):")
                for i, file_path in enumerate(restricted_files, 1):
                    print(f"  {i:2d}. {file_path}")
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æ€»æ–‡ä»¶æ•°: {len(log_files)} ä¸ª")
            print(f"   é¢„è®¡å¤§å°: {total_size/1024/1024:.2f} MB")
            print(f"   ä½¿ç”¨é…ç½®: {args.config or 'é»˜è®¤é…ç½®'}")
            print(f"   å¹¶å‘æ•°: {collector.config['max_workers']} ä¸ªçº¿ç¨‹")
            print(f"   æœç´¢æ·±åº¦: {collector.config['search_depth']} å±‚")
            print("\nğŸ’¡ ä½¿ç”¨ --verbose æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼Œå»æ‰ --dry-run å¼€å§‹å®é™…æ”¶é›†")
            
            return 0
        
        # æ­£å¸¸è¿è¡Œ
        success = collector.run()
        return 0 if success else 1
        
    except KeyboardInterrupt:
        if not args.quiet:
            print("\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
        
    except Exception as e:
        if not args.quiet:
            print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())