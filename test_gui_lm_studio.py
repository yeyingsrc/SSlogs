#!/usr/bin/env python3
"""
æµ‹è¯•GUIä¸­çš„LM Studioè¿æ¥é€»è¾‘
æ¨¡æ‹ŸGUIä¸­çš„æµ‹è¯•è¿‡ç¨‹
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_lm_studio_as_in_gui():
    """æŒ‰ç…§GUIä¸­çš„é€»è¾‘æµ‹è¯•LM Studioè¿æ¥"""
    print("ğŸ§ª æ¨¡æ‹ŸGUI LM Studioæµ‹è¯•é€»è¾‘")
    print("=" * 50)

    model_name = "openai/gpt-oss-20b"
    print(f"æµ‹è¯•æ¨¡å‹åç§°: {model_name}")

    try:
        # 1. å¯¼å…¥æ¨¡å—ï¼ˆGUIä¸­çš„å¯¼å…¥é€»è¾‘ï¼‰
        print("\n1. å¯¼å…¥æ¨¡å—...")
        from core.lm_studio_connector import LMStudioConnector, LMStudioConfig, LMStudioAPIConfig, LMStudioModelConfig
        from core.ai_config_manager import get_ai_config_manager
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")

        # 2. è·å–é…ç½®ç®¡ç†å™¨
        print("\n2. è·å–é…ç½®...")
        config_manager = get_ai_config_manager()
        lm_config = config_manager.get_lm_studio_config()
        print(f"  ä¸»æœº: {lm_config.host}:{lm_config.port}")
        print(f"  é¦–é€‰æ¨¡å‹: {lm_config.model.preferred_model or 'æœªè®¾ç½®'}")
        print(f"  æ¨¡å‹æ˜ å°„: {lm_config.model.model_mapping}")

        # 3. åˆ›å»ºè¿æ¥å™¨
        print("\n3. åˆ›å»ºè¿æ¥å™¨...")
        connector = LMStudioConnector(lm_config)
        print("âœ… è¿æ¥å™¨åˆ›å»ºæˆåŠŸ")

        # 4. æ£€æŸ¥è¿æ¥
        print("\n4. æ£€æŸ¥LM Studioè¿æ¥...")
        if connector.check_connection():
            print(f"âœ… è¿æ¥æˆåŠŸï¼")
            print(f"  å¯ç”¨æ¨¡å‹: {len(connector.available_models)}ä¸ª")
            print(f"  å½“å‰æ¨¡å‹: {connector.current_model}")

            # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
            if connector.available_models:
                print("  ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                for model in connector.available_models[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"    â€¢ {model}")
                if len(connector.available_models) > 5:
                    print(f"    ... è¿˜æœ‰ {len(connector.available_models) - 5} ä¸ªæ¨¡å‹")
        else:
            print("âŒ è¿æ¥å¤±è´¥")
            return False

        # 5. æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
        print(f"\n5. æ£€æŸ¥æ¨¡å‹ '{model_name}' æ˜¯å¦å¯ç”¨...")
        actual_model_id = lm_config.get_actual_model_id(model_name)
        print(f"  æ˜ å°„åçš„å®é™…ID: {actual_model_id}")

        if actual_model_id in connector.available_models:
            print(f"âœ… æ¨¡å‹ '{actual_model_id}' å¯ç”¨")
        else:
            print(f"âŒ æ¨¡å‹ '{actual_model_id}' ä¸å¯ç”¨")
            print("  å¯ç”¨æ¨¡å‹:")
            for model in connector.available_models:
                print(f"    â€¢ {model}")
            return False

        # 6. æµ‹è¯•æ¨¡å‹å“åº”
        print(f"\n6. æµ‹è¯•æ¨¡å‹å“åº”...")
        try:
            from core.lm_studio_connector import ChatMessage

            test_result = connector.chat_completion(
                messages=[ChatMessage(
                    role="user",
                    content="ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ç¡®è®¤è¿æ¥æ­£å¸¸"
                )],
                model=model_name  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
            )

            if test_result:
                print(f"âœ… æ¨¡å‹å“åº”æˆåŠŸï¼")
                print(f"  å“åº”: {test_result[:100]}...")
                return True
            else:
                print("âŒ æ¨¡å‹å“åº”å¤±è´¥")
                return False

        except Exception as e:
            print(f"âŒ æ¨¡å‹å“åº”å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…ç›¸å…³ä¾èµ–:")
        print("  pip install requests aiohttp")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_direct_request():
    """ç›´æ¥ä½¿ç”¨requestsæµ‹è¯•ï¼ˆç»•è¿‡è¿æ¥å™¨ï¼‰"""
    print("\n" + "=" * 50)
    print("ğŸ”— ç›´æ¥APIæµ‹è¯•")
    print("=" * 50)

    import requests

    model_name = "openai/gpt-oss-20b"
    url = "http://127.0.0.1:1234/v1/chat/completions"

    payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ç¡®è®¤è¿æ¥æ­£å¸¸"}
        ],
        "temperature": 0.3,
        "max_tokens": 50
    }

    try:
        print(f"å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")

        response = requests.post(url, json=payload, timeout=10)

        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            print(f"âœ… ç›´æ¥APIæµ‹è¯•æˆåŠŸï¼")
            print(f"  å“åº”: {content}")
            return True
        else:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
            print(f"  å“åº”: {response.text}")
            return False

    except Exception as e:
        print(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GUI LM Studio è¿æ¥æµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•GUIé€»è¾‘
    gui_success = test_lm_studio_as_in_gui()

    # æµ‹è¯•ç›´æ¥API
    api_success = test_direct_request()

    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  GUIé€»è¾‘æµ‹è¯•: {'âœ… æˆåŠŸ' if gui_success else 'âŒ å¤±è´¥'}")
    print(f"  ç›´æ¥APIæµ‹è¯•: {'âœ… æˆåŠŸ' if api_success else 'âŒ å¤±è´¥'}")

    if gui_success and api_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LM Studioåº”è¯¥å¯ä»¥åœ¨GUIä¸­æ­£å¸¸å·¥ä½œã€‚")
    elif api_success and not gui_success:
        print("\nâš ï¸ ç›´æ¥APIæ­£å¸¸ï¼Œä½†GUIé€»è¾‘æœ‰é—®é¢˜ã€‚")
        print("å»ºè®®æ£€æŸ¥:")
        print("  â€¢ é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½")
        print("  â€¢ æ¨¡å‹åç§°æ˜ å°„æ˜¯å¦æ­£ç¡®")
        print("  â€¢ è¿æ¥å™¨å®ç°æ˜¯å¦æœ‰é—®é¢˜")
    else:
        print("\nâŒ åŸºç¡€è¿æ¥æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
        print("  â€¢ LM Studioæ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("  â€¢ ç«¯å£1234æ˜¯å¦å¯ç”¨")
        print("  â€¢ æ¨¡å‹æ˜¯å¦å·²åŠ è½½")

if __name__ == "__main__":
    main()