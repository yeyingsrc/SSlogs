# SSlogs è‡ªå®šä¹‰é…ç½®æŒ‡å—

## æ¦‚è¿°

SSlogs AIç³»ç»Ÿæ”¯æŒçµæ´»çš„è‡ªå®šä¹‰é…ç½®åŠŸèƒ½ï¼Œå…è®¸æ‚¨ï¼š
- è‡ªå®šä¹‰LM Studio APIåœ°å€å’Œå‚æ•°
- æ˜ å°„æ¨¡å‹åç§°ä¸ºè‡ªå®šä¹‰åç§°
- ä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼
- é€šè¿‡Webç•Œé¢ç®¡ç†é…ç½®
- æµ‹è¯•APIè¿æ¥å’Œå“åº”

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨Webç®¡ç†ç•Œé¢

```bash
# æ–¹æ³•ä¸€ï¼šä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬
./quick_start.sh

# æ–¹æ³•äºŒï¼šä½¿ç”¨Pythonè„šæœ¬
python3 start_model_manager.py
```

è®¿é—® http://127.0.0.1:8080 æ‰“å¼€ç®¡ç†ç•Œé¢

### 2. é…ç½®è‡ªå®šä¹‰è®¾ç½®

1. ç‚¹å‡»ç•Œé¢ä¸­çš„ **"âš™ï¸ é…ç½®"** æŒ‰é’®
2. åœ¨å¼¹å‡ºçš„é…ç½®å¯¹è¯æ¡†ä¸­è®¾ç½®ï¼š
   - APIåŸºç¡€URL
   - APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
   - æ¨¡å‹å‚æ•°
   - æ¨¡å‹åç§°æ˜ å°„

### 3. æµ‹è¯•APIè¿æ¥

1. ç‚¹å‡» **"ğŸ§ª APIæµ‹è¯•"** æŒ‰é’®
2. é…ç½®æµ‹è¯•å‚æ•°ï¼š
   - APIåœ°å€
   - æ¨¡å‹åç§°
   - æµ‹è¯•æ¶ˆæ¯
   - é«˜çº§å‚æ•°
3. ç‚¹å‡» **"ğŸ§ª æµ‹è¯•API"** æŸ¥çœ‹ç»“æœ

## ğŸ”§ é…ç½®é€‰é¡¹è¯¦è§£

### LM Studio APIé…ç½®

```yaml
lm_studio:
  api:
    # APIåŸºç¡€è·¯å¾„
    base_url: "http://127.0.0.1:1234/v1"

    # è‡ªå®šä¹‰ç«¯ç‚¹
    chat_endpoint: "/chat/completions"
    models_endpoint: "/models"

    # APIè®¤è¯ï¼ˆé€šå¸¸LM Studioä¸éœ€è¦ï¼‰
    api_key: ""

    # è‡ªå®šä¹‰è¯·æ±‚å¤´
    headers:
      Content-Type: "application/json"
      User-Agent: "SSlogs-AI/1.0"
```

### æ¨¡å‹é…ç½®

```yaml
lm_studio:
  model:
    # é¦–é€‰æ¨¡å‹ï¼ˆæ”¯æŒè‡ªå®šä¹‰åç§°ï¼‰
    preferred_model: "openai/gpt-oss-20b"

    # æ¨¡å‹åç§°æ˜ å°„
    model_mapping:
      "llama-3-8b-instruct": "openai/gpt-oss-20b"
      "qwen-7b-chat": "custom/security-analyzer-v1"
      "mistral-7b-instruct": "assistant/code-helper"

    # æ¨¡å‹å‚æ•°
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stream: false
    response_format:
      type: "text"
```

## ğŸŒ OpenAIå…¼å®¹API

### ä½¿ç”¨curlæµ‹è¯•

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      { "role": "system", "content": "Always answer in rhymes. Today is Thursday" },
      { "role": "user", "content": "What day is it today?" }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
  }'
```

### ä½¿ç”¨Pythonæµ‹è¯•

```python
import requests

response = requests.post(
    "http://localhost:1234/v1/chat/completions",
    headers={"Content-Type": "application/json"},
    json={
        "model": "openai/gpt-oss-20b",
        "messages": [
            {"role": "system", "content": "Always answer in rhymes. Today is Thursday"},
            {"role": "user", "content": "What day is it today?"}
        ],
        "temperature": 0.7,
        "max_tokens": -1,
        "stream": False
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

## ğŸ“± Webç•Œé¢åŠŸèƒ½

### é…ç½®ç®¡ç†
- **APIé…ç½®**: è®¾ç½®APIåœ°å€ã€å¯†é’¥ã€ç«¯ç‚¹
- **æ¨¡å‹é…ç½®**: è°ƒæ•´æ¸©åº¦ã€ä»¤ç‰Œæ•°ã€Top-Pç­‰å‚æ•°
- **æ¨¡å‹æ˜ å°„**: ç®¡ç†å®é™…æ¨¡å‹IDä¸æ˜¾ç¤ºåç§°çš„æ˜ å°„å…³ç³»

### APIæµ‹è¯•
- **è¿æ¥æµ‹è¯•**: éªŒè¯APIåœ°å€æ˜¯å¦å¯è®¿é—®
- **æ¨¡å‹æµ‹è¯•**: æµ‹è¯•ç‰¹å®šæ¨¡å‹çš„å“åº”
- **å‚æ•°è°ƒè¯•**: è°ƒæ•´æ¸©åº¦ã€ä»¤ç‰Œæ•°ç­‰å‚æ•°
- **å“åº”åˆ†æ**: æŸ¥çœ‹å“åº”æ—¶é—´ã€ä»¤ç‰Œä½¿ç”¨ç­‰ä¿¡æ¯

### å®æ—¶ç›‘æ§
- **æœåŠ¡å™¨çŠ¶æ€**: æ˜¾ç¤ºLM Studioè¿æ¥çŠ¶æ€
- **æ¨¡å‹åˆ—è¡¨**: å±•ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
- **æ€§èƒ½æŒ‡æ ‡**: ç›‘æ§å“åº”æ—¶é—´å’Œååé‡

## ğŸ”Œ APIæ¥å£

### é…ç½®ç®¡ç†API

#### è·å–é…ç½®
```http
GET /api/config
```

å“åº”ï¼š
```json
{
  "success": true,
  "config": {
    "lm_studio": {
      "api": {
        "base_url": "http://127.0.0.1:1234/v1",
        "api_key": ""
      },
      "model": {
        "preferred_model": "openai/gpt-oss-20b",
        "model_mapping": {
          "llama-3-8b-instruct": "openai/gpt-oss-20b"
        }
      }
    }
  }
}
```

#### æ›´æ–°é…ç½®
```http
POST /api/config
Content-Type: application/json

{
  "lm_studio": {
    "api": {
      "base_url": "http://127.0.0.1:1234/v1"
    },
    "model": {
      "preferred_model": "openai/gpt-oss-20b",
      "model_mapping": {
        "llama-3-8b-instruct": "openai/gpt-oss-20b"
      }
    }
  }
}
```

### æ¨¡å‹æ˜ å°„API

#### æ·»åŠ æ˜ å°„
```http
POST /api/add_model_mapping
Content-Type: application/json

{
  "actual_model_id": "llama-3-8b-instruct",
  "display_name": "openai/gpt-oss-20b"
}
```

#### åˆ é™¤æ˜ å°„
```http
POST /api/remove_model_mapping
Content-Type: application/json

{
  "actual_model_id": "llama-3-8b-instruct"
}
```

#### è·å–æ‰€æœ‰æ˜ å°„
```http
GET /api/model_mappings
```

### APIæµ‹è¯•æ¥å£

#### æµ‹è¯•OpenAIå…¼å®¹API
```http
POST /api/test_openai_api
Content-Type: application/json

{
  "api_url": "http://localhost:1234/v1/chat/completions",
  "api_key": "",
  "model_name": "openai/gpt-oss-20b",
  "system_prompt": "Always answer in rhymes. Today is Thursday",
  "test_message": "What day is it today?",
  "temperature": 0.7,
  "max_tokens": -1,
  "stream": false
}
```

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### 1. å¤šç¯å¢ƒé…ç½®

åˆ›å»ºä¸åŒç¯å¢ƒçš„é…ç½®æ–‡ä»¶ï¼š

```yaml
# config/ai_config.dev.yaml
lm_studio:
  api:
    base_url: "http://dev-server:1234/v1"
  model:
    preferred_model: "dev-model"

# config/ai_config.prod.yaml
lm_studio:
  api:
    base_url: "http://prod-server:1234/v1"
  model:
    preferred_model: "prod-model"
```

### 2. æ¨¡å‹åˆ«åç­–ç•¥

ä½¿ç”¨æœ‰æ„ä¹‰çš„æ¨¡å‹åˆ«åï¼š

```yaml
model_mapping:
  # å®‰å…¨åˆ†æä¸“ç”¨
  "llama-3-8b-instruct": "security/analyzer-v2"
  "qwen-7b-chat": "security/threat-detector-v1"

  # ä»£ç ç”Ÿæˆä¸“ç”¨
  "codellama-13b-instruct": "code/assistant-pro"
  "deepseek-coder-6.7b": "code/python-expert"

  # é€šç”¨å¯¹è¯
  "mistral-7b-instruct": "chat/general-purpose"
  "yi-34b-chat": "chat/advanced-assistant"
```

### 3. å‚æ•°é¢„è®¾

ä¸ºä¸åŒç”¨é€”åˆ›å»ºå‚æ•°é¢„è®¾ï¼š

```python
# å®‰å…¨åˆ†æé…ç½®
security_config = LMStudioModelConfig(
    temperature=0.2,    # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
    max_tokens=1024,    # é€‚ä¸­çš„è¾“å‡ºé•¿åº¦
    top_p=0.8           # ä¿å®ˆçš„é‡‡æ ·ç­–ç•¥
)

# åˆ›æ„å†™ä½œé…ç½®
creative_config = LMStudioModelConfig(
    temperature=0.9,    # é«˜æ¸©åº¦å¢åŠ åˆ›é€ æ€§
    max_tokens=2048,    # æ›´é•¿çš„è¾“å‡º
    top_p=0.95          # æ›´å¼€æ”¾çš„é‡‡æ ·
)
```

### 4. æ‰¹é‡æ¨¡å‹æ˜ å°„

```python
# æ‰¹é‡æ·»åŠ æ¨¡å‹æ˜ å°„
mappings = {
    "llama-3-8b-instruct": "openai/gpt-oss-20b",
    "llama-3-70b-instruct": "openai/gpt-oss-70b",
    "qwen-7b-chat": "custom/qwen-7b",
    "qwen-14b-chat": "custom/qwen-14b"
}

for actual_id, display_name in mappings.items():
    # è°ƒç”¨APIæ·»åŠ æ˜ å°„
    add_model_mapping(actual_id, display_name)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. APIè¿æ¥å¤±è´¥
**ç—‡çŠ¶**: APIæµ‹è¯•è¿”å›è¿æ¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥LM Studioæ˜¯å¦æ­£åœ¨è¿è¡Œ
- ç¡®è®¤APIåœ°å€å’Œç«¯å£æ­£ç¡®
- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
- éªŒè¯æ¨¡å‹æ˜¯å¦å·²åŠ è½½

#### 2. æ¨¡å‹åç§°æ˜ å°„æ— æ•ˆ
**ç—‡çŠ¶**: ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°æ—¶å‡ºç°é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®è®¤æ˜ å°„å…³ç³»é…ç½®æ­£ç¡®
- æ£€æŸ¥å®é™…æ¨¡å‹IDæ˜¯å¦å­˜åœ¨
- é‡æ–°åŠ è½½é…ç½®
- æŸ¥çœ‹æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

#### 3. é…ç½®ä¿å­˜å¤±è´¥
**ç—‡çŠ¶**: Webç•Œé¢æ— æ³•ä¿å­˜é…ç½®

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥é…ç½®æ–‡ä»¶æƒé™
- ç¡®è®¤ç£ç›˜ç©ºé—´å……è¶³
- éªŒè¯é…ç½®æ ¼å¼æ˜¯å¦æ­£ç¡®
- æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
```yaml
logging:
  level: "DEBUG"
  detailed_logging: true
  log_requests: true
  log_responses: true
```

#### 2. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
```bash
# æµ‹è¯•APIè¿æ¥
python3 cli/model_cli.py status

# æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
python3 cli/model_cli.py list

# æµ‹è¯•ç‰¹å®šæ¨¡å‹
python3 cli/model_cli.py test llama-3-8b-instruct
```

#### 3. æ£€æŸ¥é…ç½®æ–‡ä»¶
```bash
# éªŒè¯é…ç½®æ ¼å¼
python3 -c "import yaml; yaml.safe_load(open('config/ai_config.yaml'))"

# æŸ¥çœ‹å½“å‰é…ç½®
python3 cli/model_cli.py config --section lm_studio
```

## ğŸ“š ç¤ºä¾‹ä»£ç 

### ä½¿ç”¨è‡ªå®šä¹‰é…ç½®çš„å®Œæ•´ç¤ºä¾‹

```python
from core.lm_studio_connector import LMStudioConnector, LMStudioConfig, LMStudioAPIConfig, LMStudioModelConfig

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
api_config = LMStudioAPIConfig(
    base_url="http://127.0.0.1:1234/v1",
    headers={"User-Agent": "MyApp/1.0"}
)

model_config = LMStudioModelConfig(
    preferred_model="openai/gpt-oss-20b",
    model_mapping={
        "llama-3-8b-instruct": "openai/gpt-oss-20b"
    },
    temperature=0.7,
    max_tokens=2048
)

config = LMStudioConfig(
    api=api_config,
    model=model_config
)

# åˆ›å»ºè¿æ¥å™¨
connector = LMStudioConnector(config)

# æµ‹è¯•è¿æ¥
if connector.check_connection():
    print("è¿æ¥æˆåŠŸ!")

    # å‘é€èŠå¤©è¯·æ±‚
    messages = [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]

    response = connector.chat_completion(
        messages=[LMStudioConnector.ChatMessage(**msg) for msg in messages],
        model="openai/gpt-oss-20b"  # ä½¿ç”¨æ˜ å°„çš„åç§°
    )

    print(f"AIå›å¤: {response}")
else:
    print("è¿æ¥å¤±è´¥!")
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨¡å‹å‘½åè§„èŒƒ
- ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼š`security/analyzer` è€Œä¸æ˜¯ `model1`
- åŒ…å«ç”¨é€”ä¿¡æ¯ï¼š`code/python-expert` è€Œä¸æ˜¯ `python-model`
- ç‰ˆæœ¬æ§åˆ¶ï¼š`analyzer/v2.0` è€Œä¸æ˜¯ `analyzer-new`

### 2. é…ç½®ç®¡ç†
- ä¸ºä¸åŒç¯å¢ƒåˆ›å»ºä¸åŒé…ç½®æ–‡ä»¶
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç®¡ç†é…ç½®å˜æ›´
- å®šæœŸå¤‡ä»½é‡è¦é…ç½®
- è®°å½•é…ç½®å˜æ›´çš„åŸå› å’Œå½±å“

### 3. æ€§èƒ½ä¼˜åŒ–
- æ ¹æ®ç”¨é€”è°ƒæ•´æ¨¡å‹å‚æ•°
- ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚
- ç›‘æ§APIå“åº”æ—¶é—´å’Œé”™è¯¯ç‡
- åˆç†è®¾ç½®è¶…æ—¶å’Œé‡è¯•ç­–ç•¥

### 4. å®‰å…¨è€ƒè™‘
- ä¸è¦åœ¨é…ç½®ä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯
- ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨APIå¯†é’¥
- é™åˆ¶APIè®¿é—®æƒé™
- å®šæœŸæ›´æ–°å’Œå®¡æŸ¥é…ç½®

---

**é€šè¿‡è¿™ä»½æŒ‡å—ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿå……åˆ†åˆ©ç”¨SSlogsçš„è‡ªå®šä¹‰é…ç½®åŠŸèƒ½ï¼Œæ‰“é€ é€‚åˆè‡ªå·±éœ€æ±‚çš„AIåˆ†æç³»ç»Ÿï¼** ğŸš€