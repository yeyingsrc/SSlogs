#!/usr/bin/env python3
"""
è‡ªå®šä¹‰é…ç½®å’ŒOpenAIå…¼å®¹APIæ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°å’ŒAPIé…ç½®
"""

import asyncio
import json
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.lm_studio_connector import LMStudioConnector, LMStudioConfig, LMStudioAPIConfig, LMStudioModelConfig
from core.ai_config_manager import get_ai_config_manager
from core.model_manager import get_model_manager

def demo_custom_config():
    """æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®åŠŸèƒ½"""
    print("ğŸš€ SSlogs è‡ªå®šä¹‰é…ç½®æ¼”ç¤º")
    print("=" * 60)

    # 1. åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    print("\n1. åˆ›å»ºè‡ªå®šä¹‰LM Studioé…ç½®")

    api_config = LMStudioAPIConfig(
        base_url="http://127.0.0.1:1234/v1",
        chat_endpoint="/chat/completions",
        models_endpoint="/models",
        api_key="",  # LM Studioé€šå¸¸ä¸éœ€è¦APIå¯†é’¥
        headers={
            "Content-Type": "application/json",
            "User-Agent": "SSlogs-AI-Demo/1.0"
        }
    )

    model_config = LMStudioModelConfig(
        preferred_model="openai/gpt-oss-20b",  # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹åç§°
        model_mapping={
            # å°†å®é™…æ¨¡å‹IDæ˜ å°„ä¸ºè‡ªå®šä¹‰åç§°
            "llama-3-8b-instruct": "openai/gpt-oss-20b",
            "qwen-7b-chat": "custom/security-analyzer-v1",
            "mistral-7b-instruct": "assistant/code-helper"
        },
        max_tokens=2048,
        temperature=0.7,
        top_p=0.9,
        frequency_penalty=0.0,
        presence_penalty=0.0,
        stream=False,
        response_format={"type": "text"}
    )

    config = LMStudioConfig(
        host="127.0.0.1",
        port=1234,
        timeout=30,
        retry_attempts=3,
        retry_delay=1.0,
        api=api_config,
        model=model_config
    )

    print(f"âœ… è‡ªå®šä¹‰é…ç½®åˆ›å»ºå®Œæˆ")
    print(f"   APIåœ°å€: {config.api.base_url}")
    print(f"   è‡ªå®šä¹‰æ¨¡å‹åç§°: {config.model.preferred_model}")
    print(f"   æ¨¡å‹æ˜ å°„æ•°é‡: {len(config.model.model_mapping)}")

    # 2. ä½¿ç”¨é…ç½®åˆ›å»ºè¿æ¥å™¨
    print("\n2. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºè¿æ¥å™¨")
    connector = LMStudioConnector(config)

    # æ£€æŸ¥è¿æ¥
    if connector.check_connection():
        print("âœ… è¿æ¥æˆåŠŸ")
        print(f"   å¯ç”¨æ¨¡å‹: {len(connector.available_models)}ä¸ª")

        # æ˜¾ç¤ºæ¨¡å‹æ˜ å°„
        print("\n3. æ¨¡å‹åç§°æ˜ å°„:")
        for actual_id, display_name in config.model.model_mapping.items():
            print(f"   {actual_id} â†’ {display_name}")
    else:
        print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿LM Studioæ­£åœ¨è¿è¡Œ")
        return

    # 3. æµ‹è¯•OpenAIå…¼å®¹APIè°ƒç”¨
    print("\n4. æµ‹è¯•OpenAIå…¼å®¹APIè°ƒç”¨")
    test_message = [
        {"role": "system", "content": "Always answer in rhymes. Today is Thursday"},
        {"role": "user", "content": "What day is it today?"}
    ]

    try:
        response = connector.chat_completion(
            messages=[LMStudioConnector.ChatMessage(role=msg["role"], content=msg["content"]) for msg in test_message],
            model="openai/gpt-oss-20b",  # ä½¿ç”¨æ˜ å°„çš„æ¨¡å‹åç§°
            temperature=0.7,
            max_tokens=-1,
            stream=False
        )

        if response:
            print("âœ… APIè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”: {response}")
        else:
            print("âŒ APIè°ƒç”¨å¤±è´¥")
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {e}")

def demo_config_manager():
    """æ¼”ç¤ºé…ç½®ç®¡ç†å™¨åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ”§ é…ç½®ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 60)

    config_manager = get_ai_config_manager()

    # 1. è·å–å½“å‰é…ç½®
    print("\n1. è·å–å½“å‰é…ç½®")
    current_config = config_manager.get_full_config()
    lm_config = current_config.get('lm_studio', {})

    print(f"   ä¸»æœº: {lm_config.get('host', 'N/A')}")
    print(f"   ç«¯å£: {lm_config.get('port', 'N/A')}")
    print(f"   APIåœ°å€: {lm_config.get('api', {}).get('base_url', 'N/A')}")
    print(f"   é¦–é€‰æ¨¡å‹: {lm_config.get('model', {}).get('preferred_model', 'N/A')}")

    # 2. æ›´æ–°é…ç½®
    print("\n2. æ›´æ–°é…ç½®")
    new_config = {
        "lm_studio": {
            "api": {
                "base_url": "http://127.0.0.1:1234/v1",
                "chat_endpoint": "/chat/completions",
                "models_endpoint": "/models",
                "api_key": ""
            },
            "model": {
                "preferred_model": "openai/gpt-oss-20b",
                "model_mapping": {
                    "llama-3-8b-instruct": "openai/gpt-oss-20b",
                    "qwen-7b-chat": "custom/security-analyzer"
                },
                "max_tokens": 2048,
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
    }

    # æ¨¡æ‹Ÿæ›´æ–°é…ç½®ï¼ˆå®é™…ä¿å­˜éœ€è¦è°ƒç”¨APIï¼‰
    print("   æ¨¡æ‹Ÿé…ç½®æ›´æ–°:")
    print(f"   - APIåœ°å€: {new_config['lm_studio']['api']['base_url']}")
    print(f"   - é¦–é€‰æ¨¡å‹: {new_config['lm_studio']['model']['preferred_model']}")
    print(f"   - æ¨¡å‹æ˜ å°„: {len(new_config['lm_studio']['model']['model_mapping'])}ä¸ª")

def demo_model_manager():
    """æ¼”ç¤ºæ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ¤– æ¨¡å‹ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 60)

    model_manager = get_model_manager()

    # 1. è·å–æœåŠ¡å™¨çŠ¶æ€
    print("\n1. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€")
    status = model_manager.get_server_status()
    print(f"   è¿æ¥çŠ¶æ€: {'âœ… å·²è¿æ¥' if status.connected else 'âŒ æœªè¿æ¥'}")
    if status.connected:
        print(f"   æœåŠ¡å™¨åœ°å€: {status.host}:{status.port}")
        print(f"   å¯ç”¨æ¨¡å‹æ•°: {status.available_models_count}")
        print(f"   å“åº”æ—¶é—´: {status.response_time:.2f}ç§’")

    # 2. åˆ·æ–°æ¨¡å‹åˆ—è¡¨
    print("\n2. åˆ·æ–°æ¨¡å‹åˆ—è¡¨")
    try:
        models = model_manager.refresh_models()
        print(f"   å‘ç° {len(models)} ä¸ªæ¨¡å‹")

        for i, model in enumerate(models[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"   {i}. {model.name} (ID: {model.id})")
            print(f"      å…¼å®¹æ€§: {model.compatibility_score:.1f}/5.0")
            print(f"      æ¨è: {'æ˜¯' if model.recommended else 'å¦'}")
    except Exception as e:
        print(f"   âŒ åˆ·æ–°å¤±è´¥: {e}")

def demo_openai_api_format():
    """æ¼”ç¤ºOpenAIæ ¼å¼APIè°ƒç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ”Œ OpenAIå…¼å®¹APIæ¼”ç¤º")
    print("=" * 60)

    # æ¨¡æ‹Ÿcurlå‘½ä»¤
    print("\n1. ç­‰æ•ˆçš„curlå‘½ä»¤:")
    curl_command = '''curl http://localhost:1234/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [
      { "role": "system", "content": "Always answer in rhymes. Today is Thursday" },
      { "role": "user", "content": "What day is it today?" }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
  }''''

    print(curl_command)

    # 2. Pythonä»£ç ç¤ºä¾‹
    print("\n2. Pythonè°ƒç”¨ç¤ºä¾‹:")
    python_example = '''
import requests

response = requests.post(
    "http://localhost:1234/v1/chat/completions",
    headers={"Content-Type": "application/json"},
    json={
        "model": "openai/gpt-oss-20b",
        "messages": [
            {"role": "system", "content": "Always answer in rhymes. Today is Thursday"},
            {"role": "user", "content": "What day is it today?"}
        ],
        "temperature": 0.7,
        "max_tokens": -1,
        "stream": False
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
'''
    print(python_example)

def main():
    """ä¸»å‡½æ•°"""
    try:
        demo_custom_config()
        demo_config_manager()
        demo_model_manager()
        demo_openai_api_format()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        print("\nğŸ’¡ æç¤º:")
        print("1. å¯åŠ¨LM Studioå¹¶åŠ è½½æ¨¡å‹")
        print("2. ä½¿ç”¨Webç•Œé¢è¿›è¡Œäº¤äº’å¼é…ç½®: http://127.0.0.1:8080")
        print("3. é€šè¿‡APIæ¥å£ç®¡ç†é…ç½®å’Œæµ‹è¯•æ¨¡å‹")
        print("4. æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹åç§°æ˜ å°„")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()